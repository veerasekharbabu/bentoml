[core]
debug = false
usage_tracking = false
bentoml_deploy_version = {LAST_PYPI_RELEASE_VERSION}
default_docker_base_image = 

[instrument]
default_namespace = BENTOML
prometheus_multiproc_dir = {BENTOML_HOME}/prometheus_multiproc_dir

[logging]
logging_config = {BENTOML_HOME}/logging.yml
console_logging_enabled = true
file_logging_enabled = true
level = INFO
log_format = [%%(asctime)s] %%(levelname)s - %%(message)s
dev_log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
base_log_dir = {BENTOML_HOME}/logs/
prediction_log_filename = prediction.log
feedback_log_filename = feedback.log
yatai_web_server_log_filename = yatai_web_server.log

[yatai_service]
url = 
s3_signature_version = s3v4
repository_base_url = {BENTOML_HOME}/repository/
db_url = sqlite:///{BENTOML_HOME}/storage.db
s3_endpoint_url = 
default_namespace = dev
tls_root_ca_cert = 
tls_client_key = 
tls_client_cert = 
access_token = 
access_token_header = access_token

[apiserver]
default_port = 5000
enable_metrics = true
enable_feedback = true
default_timeout = 60
default_max_request_size = 20971520
default_image_input_accept_file_extensions = .jpg,.png,.jpeg,.tiff,.webp,.bmp
default_gunicorn_workers_count = 1
batch_request_header = Bentoml-Is-Batch-Request

[marshal_server]
marshal_request_header_flag = BentoML-Is-Merged-Request

[cli]

[yatai]
bento_uri_default_expiration = 3000

[tensorflow]

[pytorch]

